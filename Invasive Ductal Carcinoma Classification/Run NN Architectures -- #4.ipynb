{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Run NN Architectures -- #4.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNndyCJw+4xeyzrTH/66urR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"R0u6MOwUKHS-","colab_type":"text"},"source":["## Mount Google Drive in Colab"]},{"cell_type":"code","metadata":{"id":"AscU9XBSigLE","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b02XNhL_KKZ5","colab_type":"text"},"source":["## Perform imports"]},{"cell_type":"code","metadata":{"id":"Uy3USZXrjKOR","colab_type":"code","outputId":"839b8960-2280-4f97-c906-4e745e045d57","executionInfo":{"status":"ok","timestamp":1585944016332,"user_tz":360,"elapsed":5517,"user":{"displayName":"James Long","photoUrl":"","userId":"09696276239741213637"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import numpy as np\n","%tensorflow_version 1.x\n","import keras\n","from keras.layers import Input, Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n","from keras.models import Model\n","from keras.optimizers import SGD\n","import json"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"zwH8lZ8zKOdw","colab_type":"text"},"source":["## Load balanced dataset"]},{"cell_type":"code","metadata":{"id":"pufhu8eTGSk7","colab_type":"code","colab":{}},"source":["data = np.load('/content/drive/My Drive/Final Capstone/Balanced Data.npy')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ZLwO_wgKRvv","colab_type":"text"},"source":["## Separate image data from label data"]},{"cell_type":"code","metadata":{"id":"e6aSgTJiGSie","colab_type":"code","colab":{}},"source":["# image data\n","X = data[:, :-1]\n","# label data\n","Y = data[:, -1:]\n","del data"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4UW-N_kfKZmJ","colab_type":"text"},"source":["## Reshape image and label data and create one-hot encoding for label data"]},{"cell_type":"code","metadata":{"id":"tA8-zAohGSgv","colab_type":"code","colab":{}},"source":["# reshape image data to 50 x 50 x 3 format\n","X = np.reshape(X, (X.shape[0], 50, 50, 3), order='C')\n","# reshape label data to 1-D array\n","Y = np.reshape(Y, (Y.shape[0]))\n","# create one-hot encoding for label data\n","Y = keras.utils.to_categorical(Y, 2, 'float32')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5UAcK8G5KyGe","colab_type":"text"},"source":["## Convolutional Neural Network models\n","\n","Twelve different architectures for simple 2-D convolutional neural networks are created and fit to the training data based on a set of common hyperparameters.  After each epoch, the models are evaluated based on their accuracy in predicting the validation set.  These sets are the same as used in the random forest models.  Each architecture is run for a minimum of 5 epochs and a maximum of 20 epochs.  At the end of each epoch after the fourth epoch, the validation accuracy of the recently completed epoch is compared to the validation accuracy of the previous epoch.  If the validation accuracy decreases, the training of the model is terminated.  For each model, a dictionary recording the results of the epochs is saved as a .json file in the directory /content/drive/My Drive/Final Capstone/Model Logs/."]},{"cell_type":"code","metadata":{"id":"t1qx1eJx1vti","colab_type":"code","colab":{}},"source":["# fraction of data to use for validation--taken from the bottom of the dataset by default in Keras\n","val_split = 0.2\n","# number of filters used in the Conv2D layers\n","filters = 32\n","# kernel size used in the Conv2D layers\n","kernel_size = (3, 3)\n","# pool size used in MaxPooling2D layers\n","pool_size = (2, 2)\n","# dropout rate used in Dropout\n","dropout_rate = 0.25\n","# width of hidden layer used prior to prediction dense layer\n","hidden_dense_size = 64\n","# initial learning rate used in optimizer\n","initial_lr = 0.01"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEE2GjAQ9Au5","colab_type":"code","colab":{}},"source":["# function used to train models\n","def optimize_model(model, X, Y, val_split, initial_lr):\n","  # counter for # of epochs model has been trained on\n","  epoch_count = 0\n","  # minimum # of epochs\n","  min_epochs = 5\n","  # maximum # of epochs\n","  max_epochs = 20\n","  # initialize dictionary to save training results\n","  history_dict = None\n","  # instantiate stochastic gradient descent optimizer with initial learning rate\n","  sgd = SGD(lr=initial_lr)\n","  # save learning rate for use in later updates\n","  sgd_lr = sgd.get_config()['lr']\n","  # while loop based on model train accuracy convergence\n","  while (True):\n","    # while loop based on learning rate reduction\n","    while (True):\n","      # complile model\n","      model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n","      # save model weights before training next epoch\n","      old_weights = model.get_weights()\n","      # train model for one epoch\n","      history = model.fit(X, Y, epochs=1, validation_split=val_split, verbose=1)\n","\n","      # save results of epoch if it is the first epoch (i.e., nothing written to history_dict yet)\n","      if (history_dict == None):\n","        history_dict = history.history\n","        break\n","      # logic to determine whether to save epoch if it is not the first epoch\n","      else:\n","        # value of training loss from previous epoch\n","        last_loss = history_dict['loss'][len(history_dict['loss']) - 1]\n","        # value of training loss from epoch just trained\n","        curr_loss = history.history['loss'][0]\n","        # keep results of epoch just completed if training loss decreases or stays the same\n","        if (curr_loss <= last_loss):\n","          for key in history_dict.keys():\n","            history_dict[key].append(history.history[key][0])\n","          break\n","        # reduce learning rate if training loss increased, reset weights to previous, and rerun epoch\n","        else:\n","          sgd = SGD(lr=sgd_lr/2)\n","          sgd_lr = sgd.get_config()['lr']\n","          model.set_weights(old_weights)\n","\n","    # increment epoch count after an epoch that reduces or maintains training loss is achieved\n","    epoch_count += 1\n","\n","    # logic of whether to terminate training\n","    if (epoch_count >= min_epochs):\n","      # terminate if max numbers of epochs have been trained\n","      if (epoch_count == max_epochs):\n","        break\n","      else:\n","        # terminate if validation accuracy decreased (if min number of epochs have been trained)\n","        if (history_dict['val_acc'][epoch_count - 1] < history_dict['val_acc'][epoch_count - 2]):\n","          break\n","  # return dictionary containing training history\n","  return history_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2EAQQCJMgOXt","colab_type":"code","colab":{}},"source":["# Conv2D, Flatten, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Flatten(data_format='channels_last')(output_layer1)\n","output_layer3 = Dense(units=2, activation='softmax')(output_layer2)\n","model = Model(input_layer, output_layer3)\n","\n","output_dict = optimize_model(model, X, Y, val_split, tolerance, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_f_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_JhYLlYK9NR","colab_type":"code","colab":{}},"source":["# Conv2D, MaxPooling, Flatten, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer1)\n","output_layer3 = Flatten(data_format='channels_last')(output_layer2)\n","output_layer4 = Dense(units=2, activation='softmax')(output_layer3)\n","model = Model(input_layer, output_layer4)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_mp_f_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQEwiFisK9Lk","colab_type":"code","colab":{}},"source":["# Conv2D, MaxPooling, Flatten, Dropout, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer1)\n","output_layer3 = Flatten(data_format='channels_last')(output_layer2)\n","output_layer4 = Dropout(rate=dropout_rate)(output_layer3)\n","output_layer5 = Dense(units=2, activation='softmax')(output_layer4)\n","model = Model(input_layer, output_layer5)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_mp_f_dr_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o95KbF_qK9Hr","colab_type":"code","colab":{}},"source":["# Conv2D, Flatten, Dense, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Flatten(data_format='channels_last')(output_layer1)\n","output_layer3 = Dense(units=hidden_dense_size, activation='relu')(output_layer2)\n","output_layer4 = Dense(units=2, activation='softmax')(output_layer3)\n","model = Model(input_layer, output_layer4)\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_f_d_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JXQavYqMK9Bb","colab_type":"code","colab":{}},"source":["# Conv2D, MaxPooling, Flatten, Dense, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer1)\n","output_layer3 = Flatten(data_format='channels_last')(output_layer2)\n","output_layer4 = Dense(units=hidden_dense_size, activation='relu')(output_layer3)\n","output_layer5 = Dense(units=2, activation='softmax')(output_layer4)\n","model = Model(input_layer, output_layer5)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_mp_f_d_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2XwexVORK87O","colab_type":"code","colab":{}},"source":["# Conv2D, MaxPooling, Flatten, Dropout, Dense, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer1)\n","output_layer3 = Flatten(data_format='channels_last')(output_layer2)\n","output_layer4 = Dropout(rate=dropout_rate)(output_layer3)\n","output_layer5 = Dense(units=hidden_dense_size, activation='relu')(output_layer4)\n","output_layer6 = Dense(units=2, activation='softmax')(output_layer5)\n","model = Model(input_layer, output_layer6)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_mp_f_dr_d_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgniusl-K800","colab_type":"code","colab":{}},"source":["# Conv2D, Conv2D, Flatten, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(output_layer1)\n","output_layer3 = Flatten(data_format='channels_last')(output_layer2)\n","output_layer4 = Dense(units=2, activation='softmax')(output_layer3)\n","model = Model(input_layer, output_layer4)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_c_f_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFi4HssfK8vs","colab_type":"code","colab":{}},"source":["# Conv2D, Conv2D, MaxPooling, Flatten, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(output_layer1)\n","output_layer3 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer2)\n","output_layer4 = Flatten(data_format='channels_last')(output_layer3)\n","output_layer5 = Dense(units=2, activation='softmax')(output_layer4)\n","model = Model(input_layer, output_layer5)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_c_mp_f_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrIjUSh1K8k2","colab_type":"code","colab":{}},"source":["# Conv2D, Conv2D, MaxPooling, Flatten, Dropout, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(output_layer1)\n","output_layer3 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer2)\n","output_layer4 = Flatten(data_format='channels_last')(output_layer3)\n","output_layer5 = Dropout(rate=dropout_rate)(output_layer4)\n","output_layer6 = Dense(units=2, activation='softmax')(output_layer5)\n","model = Model(input_layer, output_layer6)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_c_mp_f_dr_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bR9BfqvFa0tN","colab_type":"code","colab":{}},"source":["# Conv2D, Conv2D, Flatten, Dense, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(output_layer1)\n","output_layer3 = Flatten(data_format='channels_last')(output_layer2)\n","output_layer4 = Dense(units=hidden_dense_size, activation='relu')(output_layer3)\n","output_layer5 = Dense(units=2, activation='softmax')(output_layer4)\n","model = Model(input_layer, output_layer5)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_c_f_d_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"umqpUNCFazcx","colab_type":"code","colab":{}},"source":["# Conv2D, Conv2D, MaxPooling, Flatten, Dense, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(output_layer1)\n","output_layer3 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer2)\n","output_layer4 = Flatten(data_format='channels_last')(output_layer3)\n","output_layer5 = Dense(units=hidden_dense_size, activation='relu')(output_layer4)\n","output_layer6 = Dense(units=2, activation='softmax')(output_layer5)\n","model = Model(input_layer, output_layer6)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_c_mp_f_d_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VaBWQTlUctnq","colab_type":"code","colab":{}},"source":["# Conv2D, Conv2D, MaxPooling, Flatten, Dropout, Dense, Dense\n","\n","input_layer = Input(shape=X.shape[1:4])\n","output_layer1 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(input_layer)\n","output_layer2 = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), data_format='channels_last', activation='relu')(output_layer1)\n","output_layer3 = MaxPooling2D(pool_size=pool_size, strides=(1, 1), data_format='channels_last')(output_layer2)\n","output_layer4 = Flatten(data_format='channels_last')(output_layer3)\n","output_layer5 = Dropout(rate=dropout_rate)(output_layer4)\n","output_layer6 = Dense(units=hidden_dense_size, activation='relu')(output_layer5)\n","output_layer7 = Dense(units=2, activation='softmax')(output_layer6)\n","model = Model(input_layer, output_layer7)\n","\n","output_dict = optimize_model(model, X, Y, val_split, initial_lr)\n","\n","with open('/content/drive/My Drive/Final Capstone/Model Logs/c_c_mp_f_dr_d_d.json', 'w') as f:\n","  json.dump(output_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yqf703tVPsSc","colab_type":"text"},"source":["In addition to simple 2-D Convolutional Neural Network architectures, transfer learning using VGG16 is explored."]},{"cell_type":"code","metadata":{"id":"D5GT9NIkQylp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}